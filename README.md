# AIM460GroupProject1

# Method Decisions & Rationale, Final Summary

This project was a comprehensive experience, full of decisions and tests to ensure the best possible model. We experimented with many different ways to enhance the effectiveness and predictability of the dataset, from transitions to feature engineering. We have experimented with many different models, each with many different hyperparameters.

We started off with a large dataset, 768 data points that extend with 8 features and a target variable. The data was messy, unreliable, and even redundant. It was not yet fit for a model. We analyzed it and found these issues. We checked the confusian matrix, searching for null data, and didn't find any.

We then checked the histograms of each feature given the target variable, where we found a lot of new information about how we must tweak the data, and this is where decisions first started to pour in. For the first time, we saw glucose level correlated the most with whether or not a patient had diabetes. We saw insulin, age, and pregnancies correlated too. We found many features had a shift to the right, so we applied log and sqrt functions depending on the severity. We found that a lot of data had impossible 0s — values stored in place of a null — and we systematically removed them.

At this point, our data looked much more normalized, and it was time to start looking into how workable the features are. So, we generated a heat matrix that communicates the correlation between all our different features. We found a strong correlation between insulin and glucose, age and pregnancies. Once again, we were presented with a decision: remove the redundancies, or try to make a ratio out of them. First, we tried to employ a ratio. Strangely, correlation to the target variable saw significant decline, so we removed that step and instead settled for removing the redundancies. For now, we just removed the redundant skin thickness feature. We would explore removing others later, when we do a VIF analysis.

Our next issue to tackle was outliers. We noticed most features had less than 2% outliers, managable by most models without having to alter the data, but insulin had a whopping 7% outlier count. We had to act on this immediately. It was time to make another decision. After a bunch of research on our options, we settled for winsorization. This basically replaces the outlier values with more reasonable values, without losing data. We ran winsorization, and we now had 0% outliers on insulin. Outliers were now manageable.

We weren't done analyzing the data yet. There was still plenty more to look at, starting with pair plots. Pair plots confirmed our theories, that our correlated datasets were seperable and will likely be very telling for a model. Our lowest correlator, bloodpressure, was not separable. It was a mess — and it would likely be more trouble than it's worth. We still wouldn't remove it quite yet, as it might still have some use if everything else works out.

Our next step was chi-squared tests, where we gained a lot of additional insight. Each feature showed correlation with the target variable. Each feature also had a p value less than .001, so they were seemingly independent. Each feature seemed appropriate to move forward.

This was until VIF analysis, where we had shocking results. Where a VIF score of 10 implies severe multicolinearity issues, we found numbers as high as 76.57 with insulin. Insulin, age, blood pressure, BMI, and glucose all suffered from severe multicolinearity. We could not move forward with all these features. Decisions had to be made. Blood pressure was the easiest to remove, every test showed that it was a poor tell of diabetic state. In order to address this, we got rid of insulin, age, blood pressure, and BMI — our worst offenders. We were left with our three most reliable features: glucose, pregnancies, and diabetes pedigree. We would eventually try reintroducing these features to make sure removing them was the best choice.

Our features weren't perfect yet, however. We still had an issue of missing values, and too many for it to be worthwhile to just remove all rows with missing values. This is where k-nn imputation came in. We imputated in for missing values, based on the nearest neighbor of each point. We also tried mean imputation, but it k-nn imputation had a better preservation of distribution.

We still needed to standardize. Our data was in vastly different ranges, and that meant that each data point was not equally representative. Glucose had a range of 155, pregnancies had a range of 4.22, and diabetes pedigree had a range of 1.15. Once we standardized, our data was finally ready to be implemented into the first model — logistic regression.

We tried a wide range of hyperparameters for logistic regression; 10 different variations, to be exact. The one that performed the best was one that addressed the class imbalance, with a balanced hyperparameter. It was our 6th configuration. It showed that we did have an imbalance issue — one we should try addressing other ways before continuing.

So we looked at two options: SMOTE, and random undersampling. We expected SMOTE to perform, but in hindsight, our dataset didn't meet the properties that make SMOTE ideal, such as a large class imbalance (up to 1:10), or a large dataset. We were looking at a dataset of around 800, with 1.87:1 ratio, and our relationships were mostly linear. Our original model, using a balanced hyperparameter, actually performed the best. Random undersampling performed the second best, and SMOTE performed the worst, likely just introducing noise and unrealistic data. Still, for the sake of the project, we left the dataset SMOTE-balanced. It was not a significant difference either way.

Next, we analyzed the ROC curve and the AUC. We got an AUC of .802, representing effective performance where, if a diabetic patient and a nondiabetic patient are randomly selected, there is an 80% chance that the diabetic patient will be ranked higher. This is clinically significant, and shows our model is performing. The ROC curve demonstrated that we could accomplish anywhere from 80% sensitivity with 20% false positive, to 95% sensitivity with 60% false positive. False positives are definitely better than false negatives, as a false negative can endanger ones health, so high sensitivity is good.

Now that we had a decent model going, it was time to loop back to feature engineering to make sure we can not improve our model in any reasonable way. Our first test was to see if we were correct in removing insulin. It was a powerful indicator, it just suffered gravely from multicolinearity. So, we added back in insulin, and ran the model again. The difference was miniscule — definitely not enough to add a dimension of complexity to our model, so we decided to leave it out.

We decided to check if it was worth swapping out pregnancies for age. It sounds more general to go with age, so perhaps age was the better. It turned out not to be true. The model performed much worse with this swap, so we could conclude that pregnancies was the more telling feature.

This presses another question: Is it worth it to remove these features, or can we get more information making a ratio out of them. So we made a glucose insulin ratio, took the logarithm of that, and tested it out as a feature. Once again, the model performed worse across the metrics, showing what we had initially was superior.
Model Evaluation Report

After preparing and refining the dataset, we evaluated a range of models. Each model was carefully tuned using cross-validation and GridSearch, and performance was compared on the test set. The following sections summarize the models, their configurations, and the insights gained.

The first alternative to Logistic Regression was a Support Vector Machine. An RBF kernel was applied to capture potential non-linear patterns that Logistic Regression might miss. Hyperparameters including C, gamma, kernel type, and class weighting were tuned using GridSearchCV with five-fold stratified cross-validation. The best configuration was C equal to 0.1, gamma equal to 0.3, an RBF kernel, and class weight set to balanced. On the test set, the SVM achieved an accuracy of 0.7143, precision of 0.5735, recall of 0.7222, F1-score of 0.6393, and an AUC of 0.7878. The SVM provided higher recall compared to Logistic Regression, meaning it identified more diabetic patients, but it underperformed overall. The large number of support vectors also suggested that the model was overly complex without stronger generalization.

We next evaluated k-Nearest Neighbors, a simple instance-based learner that predicts outcomes based on the closest observations. Hyperparameters for the number of neighbors, weights, and distance metrics were tuned using GridSearchCV with five-fold stratified cross-validation. The best configuration was nine neighbors with Euclidean distance and uniform weights. On the test set, k-NN achieved an accuracy of 0.6818, precision of 0.5581, recall of 0.4444, F1-score of 0.4948, and an AUC of 0.7095. This model performed the worst overall. It struggled with class imbalance, as it lacks mechanisms to address this issue, and it was highly sensitive to noisy or sparse data even after scaling.

We then applied Random Forest, an ensemble of decision trees designed to improve robustness and capture non-linear relationships. Parameters tuned included depth, leaf size, split rules, number of trees, and class weighting. The best configuration was 300 trees, unrestricted maximum depth, a minimum of four samples per leaf, and balanced class weights. On the test set, the Random Forest achieved an accuracy of 0.6883, precision of 0.5577, recall of 0.5370, F1-score of 0.5472, and an AUC of 0.7544. Despite its complexity, Random Forest did not outperform Logistic Regression. Feature importance analysis showed that Glucose was by far the most influential predictor, while the other features contributed much less.

To test whether combining weaker models could improve performance, we built a soft voting classifier using SVM, k-NN, and Random Forest. The ensemble achieved an accuracy of 0.7338, precision of 0.6444, recall of 0.5370, F1-score of 0.5859, and an AUC of 0.7617. The ensemble improved slightly compared to Random Forest and k-NN individually, but it still did not outperform Logistic Regression. The high correlation between model predictions reduced the potential benefits of ensembling.

We also tested XGBoost, a gradient boosting method known for handling class imbalance and delivering strong performance on structured datasets. Hyperparameters tuned included depth, learning rate, number of estimators, subsampling, and regularization. The best configuration used 200 estimators, a maximum depth of 6, a learning rate of 0.01, subsampling of 0.8, and scale position weight of 1.87. On the test set, XGBoost achieved an accuracy of 0.7078, precision of 0.5818, recall of 0.5926, F1-score of 0.5872, and an AUC of 0.7724. Although it performed better than some models, XGBoost still trailed behind Logistic Regression in both F1-score and AUC. Feature importance once again highlighted Glucose as the most dominant predictor.

Among all the models tested, Logistic Regression with balanced class weights demonstrated the strongest performance. It achieved the highest F1-score of 0.6552 and the highest AUC of 0.8017, both of which are especially important when evaluating imbalanced medical datasets. Logistic Regression also outperformed more complex approaches, many of which showed tendencies toward overfitting or produced correlated errors. These findings suggest that the relationships in the dataset are largely linear, making Logistic Regression the most suitable choice. Despite extensive experimentation with advanced techniques, the simpler model ultimately provided the best combination of accuracy, sensitivity, and interpretability.
